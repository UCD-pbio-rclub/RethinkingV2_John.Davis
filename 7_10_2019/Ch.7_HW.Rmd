---
title: "Ch.7 HW"
author: "John D."
date: "July 10, 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ch.7 HW

## 6E1. State the three motivating criteria that define information entropy. Try to express each in your own words.

1. Uncertainty is continous
2. More events leads to more uncertainty
3. Uncertainty increases as more combinations of events are added.

## 6E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

```{r}
p <- c(0.3, 0.7)
-sum(p*log(p))
```

## 6E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?

```{r}
p <- c(0.2, 0.25, 0.25, 0.3)
-sum(p*log(p))
```

## 6E4. Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?

```{r}
p <- c(1/3, 1/3, 1/3)
-sum(p*log(p))
```

