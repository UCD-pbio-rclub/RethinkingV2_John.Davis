---
title: "Ch.7 HW"
author: "John D."
date: "July 23, 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rethinking)
```

### 1. Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the birb population. They have each found the following proportions of 5 important birb species:

||Birb A|Birb B|Birb C|Birb D|Birb E|
|---|---|---|---|---|---|
|Island 1|0.2|0.2|0.2|0.2|0.2|
|Island 2|0.8|0.1|0.05|0.025|0.025|
|Island 3|0.05|0.15|0.7|0.05|0.05|

#### Compute the entropy of each island’s birb distribution
```{r}
# -sum(p*log(p))
entro <- function(i) -sum(i*log(i))
dat <- data.frame(I1 = c(0.2,0.2,0.2,0.2,0.2),
                  I2 = c(0.8,0.1,0.05,0.025,0.025),
                  I3 = c(0.05,0.15,0.7,0.05,0.05)
                  )
apply(dat, 2, entro)
```

The first island has the most entropy since all its birb species are equally distributed leading to little surprise by seeing any species of birb.

#### Second, use each island’s birb distribution to predict the other two
```{r}
# sum( p*(log(p)-log(q)) )
Dists <- data.frame(matrix(NA, 3, 3))
KLD <- function(i,j) sum(i*(log(i)-log(j)))
for ( x in 1:3 ){
  for (y in 1:3){
    Dists[x,y] <- KLD( dat[,x], dat[,y])
  }
}
Dists
round(Dists,2)
round(colSums(Dists),2)
```

Island 1 predicts the others best because it has the least distance. This mostly is happening due to the high amount of entropy in the island

### 2. Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?

```{r}
d <- sim_happiness( seed=1977 , N_years=1000 )
precis(d)
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )
d2$mid <- d2$married + 1
m6.9 <- quap(
  alist(
    happiness ~ dnorm( mu , sigma ),
    mu <- a[mid] + bA*A,
    a[mid] ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
    ),
  data=d2 )
precis(m6.9,depth=2)
m6.10 <- quap(
  alist(
    happiness ~ dnorm( mu , sigma ),
    mu <- a + bA*A,
    a ~ dnorm( 0 , 1 ),
    bA ~ dnorm( 0 , 2 ),
    sigma ~ dexp(1)
    ),
  data=d2 )
precis(m6.10)
```
```{r}
compare(m6.9,m6.10)
```
```{r}
library(dagitty)
happiness_dag <- dagitty( "dag {
H -> M
A -> M
}")
impliedConditionalIndependencies( happiness_dag )
coordinates(happiness_dag) <- list( x=c(H=0,M=1,A=2) , y=c(H=0,M=0,A=0) )
plot( happiness_dag )
```

m6.9 is expected to make better predictions. m6.10 provides the correct causal inference. They disagree because m6.9 creates a collider when we condition on marrige which confounds our inference.

### 3. Reconsider the urban fox analysis from last week’s homework. Use WAIC or LOO based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:  
  1. avgfood + groupsize + area
  2. avgfood + groupsize
  3. groupsize + area
  4. avgfood
  5. area

### Can you explain the relative differences in WAIC scores, using the fox DAG from last week’s homework? Be sure to pay attention to the standard error of the score differences (dSE).

```{r}
fox_dag <- dagitty( "dag {
A -> F -> G -> W 
A -> F -> W
}")
impliedConditionalIndependencies( fox_dag )
coordinates(fox_dag) <- list( x=c(A=1,F=0,G=2,W=1) , y=c(A=0,F=1,G=1,W=2) )
plot( fox_dag )
```
```{r}
data(foxes)
d <- foxes
d$W <- standardize(d$weight)
d$A <- standardize(d$area)
d$F <- standardize(d$avgfood)
d$G <- standardize(d$groupsize)
head(d)
```

```{r}
m3.1 <- quap(
  alist(
    W ~ dnorm( mu, sigma),
    mu <- a + bF*F + bG*G + bA*A,
    a ~ dnorm(0,0.2),
    c(bF,bG,bA) ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data = d)
```
```{r}
m3.2 <- quap(
  alist(
    W ~ dnorm( mu, sigma),
    mu <- a + bF*F + bG*G,
    a ~ dnorm(0,0.2),
    c(bF,bG) ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data = d)
```
```{r}
m3.3 <- quap(
  alist(
    W ~ dnorm( mu, sigma),
    mu <- a + bG*G + bA*A,
    a ~ dnorm(0,0.2),
    c(bG,bA) ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data = d)
```
```{r}
m3.4 <- quap(
  alist(
    W ~ dnorm( mu, sigma),
    mu <- a + bF*F,
    a ~ dnorm(0,0.2),
    bF ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data = d)
```
```{r}
m3.5 <- quap(
  alist(
    W ~ dnorm( mu, sigma),
    mu <- a + bA*A,
    a ~ dnorm(0,0.2),
    bA ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data = d)
```
```{r}
compare(m3.1,m3.2,m3.3,m3.4,m3.5)
```
```{r}
plot(compare(m3.1,m3.2,m3.3,m3.4,m3.5))
```
```{r}
plot(coeftab(m3.1,m3.2,m3.3,m3.4,m3.5), pars=c("bF","bG","bA"))
```

The first three models do the best and their difference in standard error is small enough that they overlap and look equal. All contain group size. The last 2 models also do similarly. Given the DAG, if you have group size you have a decent estimate of weight since both area and food are routed through it. Also in the last 2, since groupsize is removed, area has to go through food directly to weight causing the two to be similar.