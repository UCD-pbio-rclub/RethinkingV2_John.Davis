---
title: "Ch. 7 HW"
author: "John D."
date: "July 17, 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=T, results='hide', message=F, warning=F}
library(tidyverse)
library(rethinking)
```

## 6M1. Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?

### AIC
  AIC = âˆ’2lppd + 2p  
  Assumes flat priors, approximately multivariate gaussian posterior distribution, and N >> k

### DIC
  I'm assuming DIC = D_theta_bar + 2PD  
  Allows informative priors but assumes approximately multivariate gaussian posterior distribution and N >> k
  
### WAIC
  WAIC = -2*( sum(lppd) - sum(pWAIC))  
  Makes no assumption of posterior shape or priors. Not sure about sample size and number of parameters
  
  
WAIC is the most general. To make a criterion less general you must begin to assume the shape of the posterior distribution and priors.

## 6M2. Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging?

Model selection you are picking the model with the best criterion value (lowest). This kind of selection procedure discards the information about relative model accuracy contained in the differences among the LOOCV/LOOIS/WAIC values. Model averaging is methods for combining the predictions of multiple models. I assume we weaken the effects of certain parameters.

## 6M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.

```{r}
data(cars)
set.seed(120)
m1 <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
    ),
  data=cars[1:25,]
  )
set.seed(120)
m2 <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
    ),
  data=cars
  )
WAIC(m1)
WAIC(m2)
compare(m1,m2)
```

More observations will lead higher WAIC making models with less observations look better.

## 6M4. What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

```{r}
set.seed(120)
m1 <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,1),
    b ~ dnorm(0,1),
    sigma ~ dexp(1)
    ),
  data=cars
  )
set.seed(120)
m2 <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
    ),
  data=cars
  )
WAIC(m1)
WAIC(m2)
compare(m1,m2)
```

pWAIC becomes smaller as more noise is filtered out due to being skeptic of prior values.

## 6M5. Provide an informal explanation of why informative priors reduce overfitting.

Informative priors reduce overfitting by reducing how much a model learns from an observation causing irregular observations to have a lesser effect on moving the coefficient.

## 6M6. Provide an information explanation of why overly informative priors result in underfitting.

Too informative of priors will cause the model to learn too little from the observations it is presented causing the observations to be overpowered by the stringency of the priors. This then leads to underfitting because the model is not taking advantage of all the information it is being provided.