---
title: "10_18_2019_HW"
author: "John D."
date: "October 14, 2019"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=native')
```

# 1. Consider the data(Wines2012) data table. These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model _score_, the subjective rating assigned by each judge to each wine. I recommend standardizing.  

  In this first problem, consider only variation among judges and wines. Construct index vaiables of _judge_ and _wine_ and then use these index variables to construct a linear regression model. Justify your priors. You should end up with 9 judge parameters and 20 wine parameters. Use _ulam_ instead of quap to build this model, and be sure to check the chains for convergence. If you'd rather build and model directly in Stan or PyMC3, go ahead. I just want you to use Hamiltonian Monte Carlo instead of quadratic approximation.  
  
  How do you interprest the variation among individual judges and invidual wines? Do you notice any patterns, just by plotting differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/best on average?
  
```{r}
#?Wines2012
data("Wines2012")
dat <- Wines2012
head(dat)
summary(dat)

dat_slim <- list(
  j = as.integer(as.factor(dat$judge)),
  w = as.integer(as.factor((dat$wine))),
  score_std = (dat$score - mean(dat$score)) / sd(dat$score) 
)

model_1 <- ulam(
  alist(
    score_std ~ dnorm(mu, sigma),
    mu <- judge[j] + wine[w],
    judge[j] ~ dnorm(0, 0.25),
    wine[w] ~ dnorm(0, 0.25),
    sigma ~ dexp(1)
  ),
  data = dat_slim, chains = 4, iter = 10000, cores = 4, log_lik = T
)

precis(model_1, depth = 2)

traceplot(model_1)
trankplot(model_1)

par(mfrow = c(1,1))
plot(precis(model_1, depth = 2))
```

The judges clearly vary with some always giving bad, some always giving average, and some always giving good scores. The wines were all pretty much centered around uniforms except wine 18 which received bad scores.

# 2. Now consider three features of the wines and judges:  
  1. `flight`: Whether the wine is red or white  
  2. `wine.amer`: Indicator variable for American wines
  3. `judge.amer`: Indictor variable for American judges

Use the indicator or index variables to model the influence of these features on the scores. Omit the individual judges and wine index variables from Problem 1. Do not include interaction effects yet. Again use `ulam`, justify your priors, and be sure to check the chains. What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in Problem 1.

```{r}
dat_slim_2 <- list(
  f = ifelse(dat$flight == "white", 0L, 1L),
  j = dat$judge.amer,
  w = dat$wine.amer,
  score_std = (dat$score - mean(dat$score)) / sd(dat$score)
)
dat_slim_2

model_2 <- ulam(
  alist(
    score_std ~ dnorm(mu, sigma),
    mu <- a + flight*f + wine*w + judge*j,
    a ~ dnorm(0, 0.25),
    judge ~ dnorm(0, 0.25),
    wine ~ dnorm(0, 0.25),
    flight ~ dnorm(0, 0.25),
    sigma ~ dexp(1)
  ),
  data = dat_slim_2, chains = 4, iter = 10000, cores = 4, log_lik = T
)

precis(model_2, depth = 2)

traceplot(model_2)
trankplot(model_2)

par(mfrow = c(1,1))
plot(precis(model_2, depth = 2))

```

American judges give higher scores and American wines score lower. Red vs White should basically no difference in scoring.

# 3. Now consider two-way interactions among the three features. You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator varaibles. Again use `ulam`, justify your priors, and be sure to check the chains. Explain what each interaction means. Be sure to interpret the model's predicitons on the outcome scale (mu, the expected score), not on the scale of individual parameters. You can use `link` to help with this, or just use your knowledge of linear model instead.  
  Wahd do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from Problem 1?
  
```{r}
model_3 <- ulam(
  alist(
    score_std ~ dnorm(mu, sigma),
    mu <- a + flight*f + wine*w + judge*j + judgewine*w*j + judgeflight*j*f + wineflight*w**f,
    a ~ dnorm(0, 0.25),
    judge ~ dnorm(0, 0.25),
    wine ~ dnorm(0, 0.25),
    flight ~ dnorm(0, 0.25),
    judgewine ~ dnorm(0, 0.25),
    judgeflight ~ dnorm(0, 0.25),
    wineflight ~ dnorm(0, 0.25),
    sigma ~ dexp(1)
  ),
  data = dat_slim_2, chains = 4, iter = 10000, cores = 4, log_lik = T
)

precis(model_3, depth = 2)

traceplot(model_3)
trankplot(model_3)

par(mfrow = c(1,1))
plot(precis(model_3, depth = 2))

pred_dat <- data.frame(
  w = rep(0:1, 12),
  j = rep(0:1, 12),
  f = rep(c(0,0,1,1), 3)
)

preds <- link(model_3, pred_dat)
plot(precis(list(mu = preds), depth = 2))

```

