---
title: "Chapter 4"
author: "John D."
date: "2/1/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
R.version

library("keras")
library("dplyr")
```

# Chapter 4. Fundamentals of machine learning
## 4.1. FOUR BRANCHES OF MACHINE LEARNING
### 4.1.1. Supervised learning
### 4.1.2. Unsupervised learning
### 4.1.3. Self-supervised learning
### 4.1.4. Reinforcement learning
#### Classification and regression glossary

1. Sample or input — One data point that goes into your model.
2. Prediction or output — What comes out of your model.
3. Target — The truth. What your model should ideally have predicted, according to an external source of data.
4. Prediction error or loss value — A measure of the distance between your model’s prediction and the target.
5. Classes — A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, “dog” and “cat” are the two classes.
6. Label — A specific instance of a class annotation in a classification problem. For instance, if picture #1234 is annotated as containing the class “dog,” then “dog” is a label of picture #1234.
7. Ground-truth or annotations — All targets for a dataset, typically collected by humans.
8. Binary classification — A classification task where each input sample should be categorized into two exclusive categories.
9. Multiclass classification — A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.
10. Multilabel classification — A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the “dog” label. The number of labels per image is usually variable.
11. Scalar regression — A task where the target is a continuous scalar value. Predicting house prices is a good example: the different target prices form a continuous space.
12. Vector regression — A task where the target is a set of continuous values: for example, a continuous vector. If you’re doing regression against multiple values (such as the coordinates of a bounding box in an image), then you’re doing vector regression.
13. Mini-batch or batch — A small set of samples (typically between 8 and 128) that are processed simultaneously by the model. The number of samples is often a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch is used to compute a single gradient-descent update applied to the weights of the model.

## 4.2. EVALUATING MACHINE-LEARNING MODELS
### 4.2.1. Training, validation, and test sets

#### Simple hold-out validation
```{r}
# indices <- sample(1:nrow(data), size = 0.80 * nrow(data))               
# evaluation_data  <- data[-indices, ]                                    
# training_data <- data[indices, ]                                        
# 
# model <- get_model()                                                    
# model %>% train(training_data)                                          
# validation_score <- model %>% evaluate(validation_data)                 
# 
# model <- get_model()                                                    
# model %>% train(data)                                                   
# test_score <- model %>% evaluate(test_data)    
```

#### K-fold validation

```{r}
# k <- 4
# indices <- sample(1:nrow(data))
# folds <- cut(indices, breaks = k, labels = FALSE)
# 
# validation_scores <- c()
# for (i in 1:k) {
# 
#   validation_indices <- which(folds == i, arr.ind = TRUE)
#   validation_data <- data[validation_indices,]                          
#   training_data <- data[-validation_indices,]                           
# 
#   model <- get_model()                                                  
#   model %>% train(training_data)
#   results <- model %>% evaluate(validation_data)
#   validation_scores <- c(validation_scores, results$accuracy)
# }
# 
# validation_score <- mean(validation_scores)                             
# 
# model <- get_model()                                                    
# model %>% train(data)                                                   
# results <- model %>% evaluate(test_data)                                
```

#### Iterated K-fold validation with shuffling

### 4.2.2. Things to keep in mind

1. Data representativeness
2. The arrow of time
3. Redundancy in your data

## 4.3. DATA PREPROCESSING, FEATURE ENGINEERING, AND FEATURE LEARNING
### 4.3.1. Data preprocessing for neural networks

* Vectorization
* Value normalization
  * Take small values
  * Be homogenous
    * Normalize each feature independently to have a mean of 0
    * Normalize each feature independently to have a standard deviation of 1
    
```{r}
# x <- scale(x) 
```

```{r}
# mean <- apply(train_data, 2, mean)                                     
# std <- apply(train_data, 2, sd)
# train_data <- scale(train_data, center = mean, scale = std)            
# test_data <- scale(test_data, center = mean, scale = std)
```

* Handling missing values
  * Make missing values 0 if 0 does not mean anything in the data
  * Make sure training and test have missing data if you expect it to be present
  
### 4.3.2. Feature engineering
## 4.4. OVERFITTING AND UNDERFITTING
### 4.4.1. Reducing the network’s size

```{r}
library(keras)

# bad
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
# good
model <- keras_model_sequential() %>%
  layer_dense(units = 4, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
# ugly
model <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

### 4.4.2. Adding weight regularization

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
              activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
              activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
regularizer_l1(0.001)
regularizer_l1_l2(l1 = 0.001, l2 = 0.001)
```

### 4.4.3. Adding dropout

```{r}
# layer_output <- layer_output * sample(0:1, length(layer_output),
#                                       replace = TRUE)
```

```{r}
# layer_output <- layer_output * 0.5
```

```{r}
# layer_output <- layer_output * sample(0:1, length(layer_output),      
#                                       replace = TRUE)
# layer_output <- layer_output / 0.5 
```

```{r}
# layer_dropout(rate = 0.5)
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

## 4.5. THE UNIVERSAL WORKFLOW OF MACHINE LEARNING
### 4.5.1. Defining the problem and assembling a dataset
### 4.5.2. Choosing a measure of success
### 4.5.3. Deciding on an evaluation protocol
### 4.5.4. Preparing your data
### 4.5.5. Developing a model that does better than a baseline

|Problem type| Last-layer activation| Loss function|
|---|---|---|
|Binary classification	|sigmoid	|binary_crossentropy|
|Multiclass, single-label classification	|softmax	|categorical_crossentropy|
|Multiclass, multilabel classification|	sigmoid|	binary_crossentropy|
|Regression to arbitrary values	|None|	mse|
|Regression to values between 0 and 1	|sigmoid	|mse or binary_crossentropy|

### 4.5.6. Scaling up: developing a model that overfits
### 4.5.7. Regularizing your model and tuning your hyperparameters
## 4.6. SUMMARY

* Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be.
C* hoose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?
* Determine your evaluation protocol: Hold-out validation? K-fold validation? Which portion of the data should you use for validation?
* Develop a first model that does better than a basic baseline: a model with statistical power.
* Develop a model that overfits.
* Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind.