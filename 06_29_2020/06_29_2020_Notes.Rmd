---
title: "06_29_2020_Notes"
author: "John D."
date: "6/29/2020"
output:
  github_document:
    pandoc_args: --webtex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rethinking)
library(tidyverse)
```

# Ch.14 Adventures in Covariance
## 14.2. Advanced varying slopes

```{r}
data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L*d$condition
dat <- list(
L = d$pulled_left,
tid = d$treatment,
actor = d$actor,
block_id = as.integer(d$block_id) )
m14.2 <- ulam(
  alist(
    L ~ binomial(1, p),
    logit(p) <- g[tid] + alpha[actor, tid] + beta[block_id, tid],
    # adaptive priors
    vector[4]:alpha[actor] ~ multi_normal(0, Rho_actor, sigma_actor),
    vector[4]:beta[block_id] ~ multi_normal(0, Rho_block, sigma_block),
    # fixed priors
    g[tid] ~ dnorm(0, 1),
    sigma_actor ~ dexp(1),
    Rho_actor ~ dlkjcorr(4),
    sigma_block ~ dexp(1),
    Rho_block ~ dlkjcorr(4)
  ) ,
  data = dat ,
  chains = 4 ,
  cores = 4,
  iter = 5000,
  log_lik = TRUE
)

```

```{r}
m14.3 <- ulam(
  alist(
    L ~ binomial(1, p),
    logit(p) <- g[tid] + alpha[actor, tid] + beta[block_id, tid],
    # adaptive priors - non-centered
    transpars > matrix[actor, 4]:alpha <-
      compose_noncentered(sigma_actor , L_Rho_actor , z_actor),
    transpars > matrix[block_id, 4]:beta <-
      compose_noncentered(sigma_block , L_Rho_block , z_block),
    matrix[4, actor]:z_actor ~ normal(0 , 1),
    matrix[4, block_id]:z_block ~ normal(0 , 1),
    # fixed priors
    g[tid] ~ normal(0, 1),
    vector[4]:sigma_actor ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky(2),
    vector[4]:sigma_block ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky(2),
    # compute ordinary correlation matrixes from Cholesky factors
    gq > matrix[4, 4]:Rho_actor <<-
      multiply_lower_tri_self_transpose(L_Rho_actor),
    gq > matrix[4, 4]:Rho_block <<-
      multiply_lower_tri_self_transpose(L_Rho_block)
  ) ,
  data = dat ,
  chains = 4 ,
  cores = 4 ,
  iter = 5000,
  log_lik = TRUE
)

```

```{r}
# extract n_eff values for each model
neff_nc <- precis(m14.3,3,pars=c("alpha","beta"))$n_eff
neff_c <- precis(m14.2,3,pars=c("alpha","beta"))$n_eff
plot( neff_c , neff_nc , xlab="centered (default)" ,
ylab="non-centered (cholesky)" , lwd=1.5 )
abline(a=0,b=1,lty=2)
```

```{r}
precis(m14.3,depth=3)
WAIC(m14.3)
compare(m14.2, m14.3)
```

```{r}
precis( m14.3 , depth=2 , pars=c("sigma_actor","sigma_block") )
```

```{r}
# compute mean for each actor in each treatment
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
# generate posterior predictions using link
datp <- list(
actor=rep(1:7,each=4) ,
tid=rep(1:4,times=7) ,
block_id=rep(5,times=4*7) )
p_post <- link( m14.3 , data=datp )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
# set up plot
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
xo <- 0.1 # offset distance to stagger raw data and predictions
# raw data
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28-xo , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.175
text( 1-xo , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2-xo , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3-xo , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4-xo , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
# posterior predictions
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )
lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )
}
for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )
points( 1:28+xo , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28+xo , p_mu , pch=c(1,1,16,16) )
```

```{r}
stancode(m14.3)
```

## 14.3. Instrumental variables and front doors
### 14.3.1. Instrumental variables

```{r}
library(dagitty)
dag14.3.1 <- dagitty( "dag {
Q -> E
U -> E
E -> W
U -> W
}")
coordinates(dag14.3.1) <- list( x=c(Q=0,U=2,E=1,W=3) , y=c(Q=0,U=0,E=.5,W=.5) )
drawdag( dag14.3.1 )
```

```{r}
set.seed(73)
N <- 500
U_sim <- rnorm(N)
Q_sim <- sample(1:4 , size = N , replace = TRUE)
E_sim <- rnorm(N , U_sim + 1 * Q_sim)
W_sim <- rnorm(N , U_sim + 0 * E_sim)
dat_sim <- list(W = standardize(W_sim) ,
                E = standardize(E_sim) ,
                Q = standardize(Q_sim))
```

```{r}
m14.4 <- ulam(
  alist(
    W ~ dnorm(mu , sigma),
    mu <- aW + bEW * E,
    aW ~ dnorm(0 , 0.2),
    bEW ~ dnorm(0 , 0.5),
    sigma ~ dexp(1)
  ) ,
  data = dat_sim ,
  chains = 4 ,
  cores = 4 ,
  log_lik = TRUE
)
precis(m14.4)
```

```{r}
m14.5 <- ulam(
  alist(
    c(W, E) ~ multi_normal(c(muW, muE) , Rho , Sigma),
    muW <- aW + bEW * E,
    muE <- aE + bQE * Q,
    c(aW, aE) ~ normal(0 , 0.2),
    c(bEW, bQE) ~ normal(0 , 0.5),
    Rho ~ lkj_corr(2),
    Sigma ~ exponential(1)
  ),
  data = dat_sim ,
  chains = 4 ,
  cores = 4
)
precis(m14.5 , depth = 3)
```

```{r}
m14.4x <- ulam( m14.4 , data=dat_sim , chains=4 , cores=4 )
m14.5x <- ulam( m14.5 , data=dat_sim , chains=4 , cores=4 )
```

```{r}
set.seed(73)
N <- 500
U_sim <- rnorm(N)
Q_sim <- sample(1:4 , size = N , replace = TRUE)
E_sim <- rnorm(N , U_sim + Q_sim)
W_sim <- rnorm(N ,-U_sim + 0.2 * E_sim)
dat_sim <- list(W = standardize(W_sim) ,
                E = standardize(E_sim) ,
                Q = standardize(Q_sim))
```

```{r}
dagIV <- dagitty( "dag{
E -> W
E <- U -> W
Q -> E
}")
instrumentalVariables( dagIV , exposure="E" , outcome="W" )
```

### 14.3.2. Front-door criterion

## 14.4. Social relations as correlated varying effects

```{r}
data(KosterLeckie)
```

```{r}
kl_data <- list(
  N = nrow(kl_dyads),
  N_households = max(kl_dyads$hidB),
  did = kl_dyads$did,
  hidA = kl_dyads$hidA,
  hidB = kl_dyads$hidB,
  giftsAB = kl_dyads$giftsAB,
  giftsBA = kl_dyads$giftsBA
)
m14.4 <- ulam(
  alist(
    giftsAB ~ poisson(lambdaAB),
    giftsBA ~ poisson(lambdaBA),
    log(lambdaAB) <- a + gr[hidA, 1] + gr[hidB, 2] + d[did, 1] ,
    log(lambdaBA) <- a + gr[hidB, 1] + gr[hidA, 2] + d[did, 2] ,
    a ~ normal(0, 1),
    ## gr matrix of varying effects
    vector[2]:gr[N_households] ~ multi_normal(0, Rho_gr, sigma_gr),
    Rho_gr ~ lkj_corr(4),
    sigma_gr ~ exponential(1),
    ## dyad effects
    transpars > matrix[N, 2]:d <-
      compose_noncentered(rep_vector(sigma_d, 2) , L_Rho_d , z),
    matrix[2, N]:z ~ normal(0 , 1),
    cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky(8),
    sigma_d ~ exponential(1),
    ## compute correlation matrix for dyads
    gq > matrix[2, 2]:Rho_d <<- Chol_to_Corr(L_Rho_d)
  ),
  data = kl_data ,
  chains = 4 ,
  cores = 4 ,
  iter = 2000
)

```

```{r}
precis( m14.4 , depth=3 , pars=c("Rho_gr","sigma_gr") )
```

```{r}
post <- extract.samples( m14.4 )
g <- sapply( 1:25 , function(i) post$a + post$gr[,i,1] )
r <- sapply( 1:25 , function(i) post$a + post$gr[,i,2] )
Eg_mu <- apply( exp(g) , 2 , mean )
Er_mu <- apply( exp(r) , 2 , mean )

```

```{r}
plot(exp(g[,1]),exp(r[,1]))
```

```{r}
plot(
  NULL ,
  xlim = c(0, 8.6) ,
  ylim = c(0, 8.6) ,
  xlab = "generalized giving" ,
  ylab = "generalized receiving" ,
  lwd = 1.5
)
abline(a = 0, b = 1, lty = 2)
# ellipses
library(ellipse)
for (i in 1:25) {
  Sigma <- cov(cbind(g[, i] , r[, i]))
  Mu <- c(mean(g[, i]) , mean(r[, i]))
  for (l in c(0.5)) {
    el <- ellipse(Sigma , centre = Mu , level = l)
    lines(exp(el) , col = col.alpha("black", 0.5))
  }
}
# household means
points(Eg_mu ,
       Er_mu ,
       pch = 21 ,
       bg = "white" ,
       lwd = 1.5)
```

```{r}
precis( m14.4 , depth=3 , pars=c("Rho_d","sigma_d") )
```

```{r}
dy1 <- apply( post$d[,,1] , 2 , mean )
dy2 <- apply( post$d[,,2] , 2 , mean )
plot( dy1 , dy2 )

```

## 14.5. Continuous categories and the Gaussian process

### 14.5.1. Example: Spatial autocorrelation in Oceanic tools.

```{r}
# load the distance matrix
library(rethinking)
data(islandsDistMatrix)
# display (measured in thousands of km)
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
round(Dmat,1)

```

```{r}
# linear
curve( exp(-1*x) , from=0 , to=4 , lty=2 ,
xlab="distance" , ylab="correlation" )
# squared
curve( exp(-1*x^2) , add=TRUE )
```

```{r}
data(Kline2) # load the ordinary data, now with coordinates
d <- Kline2
d$society <- 1:10 # index observations
dat_list <- list(
  T = d$total_tools,
  P = d$population,
  society = d$society,
  Dmat = islandsDistMatrix
)
m14.7 <- ulam(
  alist(
    T ~ dpois(lambda),
    lambda <- (a * P ^ b / g) * exp(k[society]),
    vector[10]:k ~ multi_normal(0 , SIGMA),
    matrix[10, 10]:SIGMA <- cov_GPL2(Dmat , etasq , rhosq , 0.01),
    c(a, b, g) ~ dexp(1),
    etasq ~ dexp(2),
    rhosq ~ dexp(0.5)
  ),
  data = dat_list ,
  chains = 4 ,
  cores = 4 ,
  iter = 2000
)
```

```{r}
precis( m14.7 , depth=3)
```

```{r}
post <- extract.samples(m14.7)
# plot the posterior median covariance function
plot(
  NULL ,
  xlab = "distance (thousand km)" ,
  ylab = "covariance" ,
  xlim = c(0, 10) ,
  ylim = c(0, 2)
)
# compute posterior mean covariance
x_seq <- seq(from = 0 ,
             to = 10 ,
             length.out = 100)
pmcov <-
  sapply(x_seq , function(x)
    post$etasq * exp(-post$rhosq * x ^ 2))
pmcov_mu <- apply(pmcov , 2 , mean)
lines(x_seq , pmcov_mu , lwd = 2)
# plot 60 functions sampled from posterior
for (i in 1:50)
  curve(post$etasq[i] * exp(-post$rhosq[i] * x ^ 2) ,
        add = TRUE ,
        col = col.alpha("black", 0.3))
```

```{r}
# compute posterior median covariance among societies
K <- matrix(0,nrow=10,ncol=10)
for ( i in 1:10 )
for ( j in 1:10 )
K[i,j] <- median(post$etasq) *
exp( -median(post$rhosq) * islandsDistMatrix[i,j]^2 )
diag(K) <- median(post$etasq) + 0.01

```

```{r}
# convert to correlation matrix
Rho <- round( cov2cor(K) , 2 )
# add row/col names for convenience
colnames(Rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(Rho) <- colnames(Rho)
Rho

```

```{r}
# scale point size to logpop
psize <- d$logpop / max(d$logpop)
psize <- exp(psize*1.5)-2
# plot raw data and labels
plot( d$lon2 , d$lat , xlab="longitude" , ylab="latitude" ,
col=rangi2 , cex=psize , pch=16 , xlim=c(-50,30) )
labels <- as.character(d$culture)
text( d$lon2 , d$lat , labels=labels , cex=0.7 , pos=c(2,4,3,3,4,1,3,2,4,2) )
# overlay lines shaded by Rho
for( i in 1:10 )
for ( j in 1:10 )
if ( i < j )
lines( c( d$lon2[i],d$lon2[j] ) , c( d$lat[i],d$lat[j] ) ,
lwd=2 , col=col.alpha("black",Rho[i,j]^2) )

```

```{r}
# compute posterior median relationship, ignoring distance
logpop.seq <- seq(from = 6 ,
                  to = 14 ,
                  length.out = 30)
lambda <-
  sapply(logpop.seq , function(lp)
    exp(post$a + post$b * lp))
lambda.median <- apply(lambda , 2 , median)
lambda.PI80 <- apply(lambda , 2 , PI , prob = 0.8)
# plot raw data and labels
plot(
  d$logpop ,
  d$total_tools ,
  col = rangi2 ,
  cex = psize ,
  pch = 16 ,
  xlab = "log population" ,
  ylab = "total tools"
)
text(
  d$logpop ,
  d$total_tools ,
  labels = labels ,
  cex = 0.7 ,
  pos = c(4, 3, 4, 2, 2, 1, 4, 4, 4, 2)
)
# display posterior predictions
lines(logpop.seq , lambda.median , lty = 2)
lines(logpop.seq , lambda.PI80[1, ] , lty = 2)
lines(logpop.seq , lambda.PI80[2, ] , lty = 2)
# overlay correlations
for (i in 1:10)
  for (j in 1:10)
    if (i < j)
      lines(
        c(d$logpop[i], d$logpop[j]) ,
        c(d$total_tools[i], d$total_tools[j]) ,
        lwd = 2 ,
        col = col.alpha("black", Rho[i, j] ^ 2)
      )
```

### 14.5.2. Example: Phylogenetic distance

```{r}
library(rethinking)
data(Primates301)
data(Primates301_nex)
# plot it using ape package - install.packages('ape') if needed
library(ape)
plot( ladderize(Primates301_nex) , type="fan" , font=1 , no.margin=TRUE ,
label.offset=1 , cex=0.5 )
```

```{r}
d <- Primates301
d$name <- as.character(d$name)
dstan <- d[ complete.cases( d$group_size , d$body , d$brain ) , ]
spp_obs <- dstan$name
```

```{r}
dat_list <- list(
  N_spp = nrow(dstan),
  M = standardize(log(dstan$body)),
  B = standardize(log(dstan$brain)),
  G = standardize(log(dstan$group_size)),
  Imat = diag(nrow(dstan))
)
m14.8 <- ulam(
  alist(
    B ~ multi_normal(mu , SIGMA),
    mu <- a + bM * M + bG * G,
    matrix[N_spp, N_spp]:SIGMA <- Imat * sigma_sq,
    a ~ normal(0 , 1),
    c(bM, bG) ~ normal(0 , 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list ,
  chains = 4 ,
  cores = 4
)
precis(m14.8)
```

```{r}
library(ape)
tree_trimmed <- keep.tip( Primates301_nex, spp_obs )
Rbm <- corBrownian( phy=tree_trimmed )
V <- vcv(Rbm)
Dmat <- cophenetic( tree_trimmed )
plot( Dmat , V , xlab="phylogenetic distance" , ylab="covariance" )
```

```{r}
image(V)
```

```{r}
# put species in right order
dat_list$V <- V[spp_obs , spp_obs]
# convert to correlation matrix
dat_list$R <- dat_list$V / max(V)
# Brownian motion model
m14.9 <- ulam(
  alist(
    B ~ multi_normal(mu , SIGMA),
    mu <- a + bM * M + bG * G,
    matrix[N_spp, N_spp]:SIGMA <- R * sigma_sq,
    a ~ normal(0 , 1),
    c(bM, bG) ~ normal(0 , 0.5),
    sigma_sq ~ exponential(1)
  ),
  data = dat_list ,
  chains = 4 ,
  cores = 4
)
precis(m14.9)
```

```{r}
# add scaled and reordered distance matrix
dat_list$Dmat <- Dmat[spp_obs , spp_obs] / max(Dmat)
m14.10 <- ulam(
  alist(
    B ~ multi_normal(mu , SIGMA),
    mu <- a + bM * M + bG * G,
    matrix[N_spp, N_spp]:SIGMA <-
      cov_GPL1(Dmat , etasq , rhosq , 0.01),
    a ~ normal(0, 1),
    c(bM, bG) ~ normal(0, 0.5),
    etasq ~ half_normal(1, 0.25),
    rhosq ~ half_normal(3, 0.25)
  ),
  data = dat_list ,
  chains = 4 ,
  cores = 4
)
precis(m14.10)
```

```{r}
post <- extract.samples(m14.10)
plot(
  NULL ,
  xlim = c(0, max(dat_list$Dmat)) ,
  ylim = c(0, 1.5) ,
  xlab = "phylogenetic distance" ,
  ylab = "covariance"
)
# posterior
for (i in 1:30)
  curve(post$etasq[i] * exp(-post$rhosq[i] * x) ,
        add = TRUE ,
        col = rangi2)
# prior mean and 89% interval
eta <- abs(rnorm(1e3, 1, 0.25))
rho <- abs(rnorm(1e3, 3, 0.25))
d_seq <- seq(from = 0,
             to = 1,
             length.out = 50)
K <- sapply(d_seq , function(x)
  eta * exp(-rho * x))
lines(d_seq , colMeans(K) , lwd = 2)
shade(apply(K, 2, PI) , d_seq)
text(0.5 , 0.5 , "prior")
text(0.2 , 0.1 , "posterior" , col = rangi2)
```

