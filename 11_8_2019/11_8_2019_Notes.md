---
title: "11_8_2019_Notes"
author: "John D."
date: "November 8, 2019"
output: 
  html_document: 
    keep_md: yes
---




```r
library(tidyverse)
```

```
## -- Attaching packages ---------------------------------------------------------- tidyverse 1.2.1 --
```

```
## v ggplot2 3.2.1     v purrr   0.3.3
## v tibble  2.1.3     v dplyr   0.8.3
## v tidyr   1.0.0     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.4.0
```

```
## -- Conflicts ------------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
```

```r
library(rethinking)
```

```
## Loading required package: rstan
```

```
## Loading required package: StanHeaders
```

```
## rstan (Version 2.19.2, GitRev: 2e1f913d3ca3)
```

```
## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)
```

```
## For improved execution time, we recommend calling
## Sys.setenv(LOCAL_CPPFLAGS = '-march=native')
## although this causes Stan to throw an error on a few processors.
```

```
## 
## Attaching package: 'rstan'
```

```
## The following object is masked from 'package:tidyr':
## 
##     extract
```

```
## Loading required package: parallel
```

```
## Loading required package: dagitty
```

```
## rethinking (Version 1.91)
```

```
## 
## Attaching package: 'rethinking'
```

```
## The following object is masked from 'package:purrr':
## 
##     map
```

```
## The following object is masked from 'package:stats':
## 
##     rstudent
```

```r
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=native')
```

# 11 God Spiked the Integers
## 11.1. Binomial regression
### 11.1.1. Logistic regression: Prosocial chimpanzees


```r
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2 * d$condition
xtabs(~ treatment + prosoc_left + condition , d)
```

```
## , , condition = 0
## 
##          prosoc_left
## treatment   0   1
##         1 126   0
##         2   0 126
##         3   0   0
##         4   0   0
## 
## , , condition = 1
## 
##          prosoc_left
## treatment   0   1
##         1   0   0
##         2   0   0
##         3 126   0
##         4   0 126
```


```r
m11.1 <- quap(alist(pulled_left ~ dbinom(1 , p) ,
                    logit(p) <- a ,
                    a ~ dnorm(0 , 10)) , data = d)

set.seed(1999)
prior <- extract.prior(m11.1 , n = 1e4)
p <- inv_logit(prior$a)
dens(p , adj = 0.1)

m11.1 <- quap(alist(pulled_left ~ dbinom(1 , p) ,
                    logit(p) <- a ,
                    a ~ dnorm(0 , 1.5)) , data = d)

set.seed(1999)
prior <- extract.prior(m11.1 , n = 1e4)
p <- inv_logit(prior$a)
dens(p , adj = 0.1, add = T, col = "blue")
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-3-1.png)<!-- -->


```r
m11.2 <- quap(alist(
  pulled_left ~ dbinom(1 , p) ,
  logit(p) <- a + b[treatment] ,
  a ~ dnorm(0 , 1.5),
  b[treatment] ~ dnorm(0 , 10)
) ,
data = d)

set.seed(1999)
prior <- extract.prior(m11.2 , n = 1e4)
p <- sapply(1:4 , function(k)
  inv_logit(prior$a + prior$b[, k]))
dens(abs(p[, 1] - p[, 2]) , adj = 0.1)

m11.3 <- quap(alist(
  pulled_left ~ dbinom(1 , p) ,
  logit(p) <- a + b[treatment] ,
  a ~ dnorm(0 , 1.5),
  b[treatment] ~ dnorm(0 , 0.5)
) ,
data = d)
set.seed(1999)
prior <- extract.prior(m11.3 , n = 1e4)
p <- sapply(1:4 , function(k)
  inv_logit(prior$a + prior$b[, k]))
dens(abs(p[, 1] - p[, 2]) ,
     adj = 0.1,
     add = T,
     col = "blue")
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-4-1.png)<!-- -->

```r
mean(abs(p[, 1] - p[, 2]))
```

```
## [1] 0.09838663
```

```r
mean(abs(p[, 1] - p[, 3]))
```

```
## [1] 0.09791694
```

```r
mean(abs(p[, 1] - p[, 4]))
```

```
## [1] 0.09856087
```

```r
mean(abs(p[, 2] - p[, 3]))
```

```
## [1] 0.09879503
```

```r
mean(abs(p[, 2] - p[, 4]))
```

```
## [1] 0.0985305
```

```r
mean(abs(p[, 3] - p[, 4]))
```

```
## [1] 0.09815794
```


```r
# prior trimmed data list
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)
# particles in 11-dimensional space
m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom(1 , p) ,
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm(0 , 1.5),
    b[treatment] ~ dnorm(0 , 0.5)
  ) ,
  data = dat_list,
  chains = 4,
  log_lik=TRUE
)
```

```
## 
## SAMPLING FOR MODEL '80e2b6267e3dc4ff0c2916d0cf0879e8' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.66 seconds (Warm-up)
## Chain 1:                0.744 seconds (Sampling)
## Chain 1:                1.404 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '80e2b6267e3dc4ff0c2916d0cf0879e8' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.759 seconds (Warm-up)
## Chain 2:                0.706 seconds (Sampling)
## Chain 2:                1.465 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '80e2b6267e3dc4ff0c2916d0cf0879e8' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.663 seconds (Warm-up)
## Chain 3:                0.651 seconds (Sampling)
## Chain 3:                1.314 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '80e2b6267e3dc4ff0c2916d0cf0879e8' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.644 seconds (Warm-up)
## Chain 4:                0.544 seconds (Sampling)
## Chain 4:                1.188 seconds (Total)
## Chain 4:
```

```r
precis(m11.4 , depth = 2)
```

```
##             mean        sd        5.5%       94.5%     n_eff     Rhat
## a[1] -0.44662951 0.3212620 -0.95066238  0.06345093  646.1365 1.006385
## a[2]  3.92555845 0.7748581  2.79870885  5.19339940 1272.0420 1.001519
## a[3] -0.75580528 0.3381962 -1.30777047 -0.21802458  782.1167 1.006893
## a[4] -0.75012741 0.3297006 -1.27397722 -0.22916035  674.1171 1.006087
## a[5] -0.44065330 0.3140066 -0.94178658  0.06137972  800.5656 1.006077
## a[6]  0.48443470 0.3259878 -0.03715598  1.01053287  826.2722 1.006794
## a[7]  1.95778498 0.4181679  1.30450901  2.62047870  965.5209 1.003335
## b[1] -0.04355881 0.2783216 -0.51699325  0.36488910  715.0729 1.011344
## b[2]  0.47616555 0.2839300  0.01648295  0.93302454  735.5020 1.004997
## b[3] -0.38455012 0.2809016 -0.84347442  0.04779866  720.9321 1.006081
## b[4]  0.37235142 0.2766925 -0.07208403  0.81423277  664.8869 1.010324
```

```r
post <- extract.samples(m11.4)
p_left <- inv_logit(post$a)
row_labels <- paste("Chimpanzee", 1:7)
plot(precis(as.data.frame(p_left)) , xlim = c(0, 1), labels = row_labels)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-5-1.png)<!-- -->


```r
labs <- c("R/N","L/N","R/P","L/P")
plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs )
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-6-1.png)<!-- -->


```r
diffs <- list(db13 = post$b[, 1] - post$b[, 3],
              db24 = post$b[, 2] - post$b[, 4])
labs <- c("Right Differences", "Left Differences")
plot(precis(diffs), labels = labs)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-7-1.png)<!-- -->


```r
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
pl[1,]
```

```
##         1         2         3         4 
## 0.3333333 0.5000000 0.2777778 0.5555556
```


```r
plot(NULL,
     xlim=c(1,28),
     ylim=c(0,1),
     xlab="" ,
     ylab="proportion left lever",
     xaxt="n",
     yaxt="n" )
axis(2, at=c(0,0.5,1), labels=c(0,0.5,1))
abline( h=0.5 , lty=2 )
for ( j in 1:7 ){
  abline( v=(j-1)*4+4.5 , lwd=0.5 )
}
for ( j in 1:7 ){
  text((j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
}
for ( j in (1:7)[-2] ) {
  lines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
  lines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28 , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.01
text( 1 , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2 , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3 , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4 , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
mtext( "Proportions\n" )

dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link_ulam( m11.4 , data=dat )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
p_post <- colMeans(p_post)

for ( j in 1:7) {
  lines( (j-1)*4+c(1,3) , p_post[(j-1)*4+c(1,3)] , lwd=2 , col="black" )
  lines( (j-1)*4+c(2,4) , p_post[(j-1)*4+c(2,4)] , lwd=2 , col="black" )
}
points( 1:28 , p_post[1:28] , pch=16 , col="white" , cex=1.7 )
points( 1:28 , p_post[1:28] , pch=c(1,1,16,16) , col="black" , lwd=2 )
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-9-1.png)<!-- -->


```r
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

dat_list2 <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  side = d$side,
  cond = d$cond )

m11.5 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + bs[side] + bc[cond] ,
    a[actor] ~ dnorm( 0 , 1.5 ),
    bs[side] ~ dnorm( 0 , 0.5 ),
    bc[cond] ~ dnorm( 0 , 0.5 )
) ,
data=dat_list2 , chains=4 , log_lik=TRUE )
```

```
## 
## SAMPLING FOR MODEL '1e8ca8c75419cf9bc49e27f99cb338e6' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 1.108 seconds (Warm-up)
## Chain 1:                0.99 seconds (Sampling)
## Chain 1:                2.098 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '1e8ca8c75419cf9bc49e27f99cb338e6' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0.001 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 1.056 seconds (Warm-up)
## Chain 2:                1.019 seconds (Sampling)
## Chain 2:                2.075 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '1e8ca8c75419cf9bc49e27f99cb338e6' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 1.15 seconds (Warm-up)
## Chain 3:                0.965 seconds (Sampling)
## Chain 3:                2.115 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '1e8ca8c75419cf9bc49e27f99cb338e6' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 1.129 seconds (Warm-up)
## Chain 4:                0.97 seconds (Sampling)
## Chain 4:                2.099 seconds (Total)
## Chain 4:
```


```r
compare( m11.5 , m11.4 , func=LOO )
```

```
##           PSIS pPSIS    dPSIS    weight       SE      dSE
## m11.5 530.9591     0 0.000000 0.6449408 19.11233       NA
## m11.4 532.1528     0 1.193748 0.3550592 18.96732 1.314614
```

### Overthinkingg: Adding log-probability calculations to a Stan model


```r
post <- extract.samples( m11.4 , clean=FALSE )
str(post)
```

```
## List of 4
##  $ log_lik: num [1:2000, 1:504] -0.37 -0.455 -0.71 -0.5 -0.497 ...
##  $ a      : num [1:2000, 1:7] -0.649 -0.186 -0.169 -0.181 -0.687 ...
##  $ b      : num [1:2000, 1:4] -0.155 -0.365 0.203 -0.252 0.246 ...
##  $ lp__   : num [1:2000(1d)] -266 -269 -267 -264 -269 ...
##  - attr(*, "source")= chr "ulam posterior: 2000 samples from m11.4"
```

```r
stancode(m11.4)
```

```
## data{
##     int pulled_left[504];
##     int treatment[504];
##     int actor[504];
## }
## parameters{
##     vector[7] a;
##     vector[4] b;
## }
## model{
##     vector[504] p;
##     b ~ normal( 0 , 0.5 );
##     a ~ normal( 0 , 1.5 );
##     for ( i in 1:504 ) {
##         p[i] = a[actor[i]] + b[treatment[i]];
##         p[i] = inv_logit(p[i]);
##     }
##     pulled_left ~ binomial( 1 , p );
## }
## generated quantities{
##     vector[504] log_lik;
##     vector[504] p;
##     for ( i in 1:504 ) {
##         p[i] = a[actor[i]] + b[treatment[i]];
##         p[i] = inv_logit(p[i]);
##     }
##     for ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] );
## }
```

```r
m11.4_stan_code <- stancode(m11.4)
```

```
## data{
##     int pulled_left[504];
##     int treatment[504];
##     int actor[504];
## }
## parameters{
##     vector[7] a;
##     vector[4] b;
## }
## model{
##     vector[504] p;
##     b ~ normal( 0 , 0.5 );
##     a ~ normal( 0 , 1.5 );
##     for ( i in 1:504 ) {
##         p[i] = a[actor[i]] + b[treatment[i]];
##         p[i] = inv_logit(p[i]);
##     }
##     pulled_left ~ binomial( 1 , p );
## }
## generated quantities{
##     vector[504] log_lik;
##     vector[504] p;
##     for ( i in 1:504 ) {
##         p[i] = a[actor[i]] + b[treatment[i]];
##         p[i] = inv_logit(p[i]);
##     }
##     for ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] );
## }
```

```r
m11.4_stan <- stan( model_code=m11.4_stan_code , data=dat_list , chains=4 )
```

```
## recompiling to avoid crashing R session
```

```r
compare( m11.4_stan , m11.4 )
```

```
## Warning in compare(m11.4_stan, m11.4): Not all model fits of same class.
## This is usually a bad idea, because it implies they were fit by different algorithms.
## Check yourself, before you wreck yourself.
```

```
##                WAIC    pWAIC      dWAIC    weight       SE       dSE
## m11.4_stan 532.0159 8.374586 0.00000000 0.5092235 18.91315        NA
## m11.4      532.0897 8.431695 0.07379669 0.4907765 18.94519 0.1364307
```

### 11.1.2. Relative shark and absolute penguin


```r
post <- extract.samples(m11.4)
mean( exp(post$b[,4]-post$b[,2]) )
```

```
## [1] 0.9360553
```

### 11.1.3. Aggregated binomial: Chimpanzees again, condensed


```r
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
  d$pulled_left ,
  list( treatment=d$treatment,
        actor=d$actor,
        side=d$side,
        cond=d$cond ) ,
  sum )
colnames(d_aggregated)[5] <- "left_pulls"
head(d_aggregated,8)
```

```
##   treatment actor side cond left_pulls
## 1         1     1    1    1          6
## 2         1     2    1    1         18
## 3         1     3    1    1          5
## 4         1     4    1    1          6
## 5         1     5    1    1          6
## 6         1     6    1    1         14
## 7         1     7    1    1         14
## 8         2     1    2    1          9
```


```r
dat <- with(
  d_aggregated ,
  list(
    left_pulls = left_pulls,
    treatment = treatment,
    actor = actor,
    side = side,
    cond = cond
    )
  )

m11.6 <- ulam(
  alist(
    left_pulls ~ dbinom(18 , p) ,
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm(0 , 1.5) ,
    b[treatment] ~ dnorm(0 , 0.5)
    ),
  data = dat ,
  chains = 4 ,
  log_lik = TRUE
  )
```

```
## 
## SAMPLING FOR MODEL 'fe7afceed303d5d7dc6e181ecdd9c372' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.072 seconds (Warm-up)
## Chain 1:                0.066 seconds (Sampling)
## Chain 1:                0.138 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'fe7afceed303d5d7dc6e181ecdd9c372' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.071 seconds (Warm-up)
## Chain 2:                0.069 seconds (Sampling)
## Chain 2:                0.14 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'fe7afceed303d5d7dc6e181ecdd9c372' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.068 seconds (Warm-up)
## Chain 3:                0.075 seconds (Sampling)
## Chain 3:                0.143 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'fe7afceed303d5d7dc6e181ecdd9c372' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.067 seconds (Warm-up)
## Chain 4:                0.051 seconds (Sampling)
## Chain 4:                0.118 seconds (Total)
## Chain 4:
```


```r
compare( m11.6 , m11.4 , func=LOO )
```

```
## Warning in compare(m11.6, m11.4, func = LOO): Different numbers of observations found for at least two models.
## Information criteria only valid for comparing models fit to exactly same observations.
## Number of observations for each model:
## m11.6 28 
## m11.4 504
```

```
## Warning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are high (>0.5). Use PSISk to inspect individual points.
```

```
##           PSIS pPSIS   dPSIS       weight        SE      dSE
## m11.6 113.7118     0   0.000 1.000000e+00  8.426122       NA
## m11.4 532.1528     0 418.441 1.369902e-91 18.967322 9.453018
```


```r
# deviance of aggregated 6-in-9
-2*dbinom(6,9,0.2,log=TRUE)
```

```
## [1] 11.79048
```

```r
# deviance of dis-aggregated
-2*sum(dbern(c(1,1,1,1,1,1,0,0,0),0.2,log=TRUE))
```

```
## [1] 20.65212
```


```r
( k <- PSISk(m11.6) )
```

```
## Warning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are high (>0.5). Use PSISk to inspect individual points.
```

```
##  [1] 0.15 0.24 0.31 0.19 0.47 0.52 0.50 0.25 0.14 0.61 0.29 0.33 0.56 0.21
## [15] 0.35 0.43 0.51 0.46 0.43 0.36 0.66 0.42 0.11 0.42 0.27 0.39 0.42 0.38
```

### 11.1.4. Aggregated binomial: Graduate school admissions


```r
data(UCBadmit)
d <- UCBadmit

d
```

```
##    dept applicant.gender admit reject applications
## 1     A             male   512    313          825
## 2     A           female    89     19          108
## 3     B             male   353    207          560
## 4     B           female    17      8           25
## 5     C             male   120    205          325
## 6     C           female   202    391          593
## 7     D             male   138    279          417
## 8     D           female   131    244          375
## 9     E             male    53    138          191
## 10    E           female    94    299          393
## 11    F             male    22    351          373
## 12    F           female    24    317          341
```

```r
d$gid <- ifelse( d$applicant.gender=="male" , 1 , 2 )

m11.7 <- quap(
  alist(
    admit ~ dbinom( applications , p ) ,
    logit(p) <- a[gid] ,
    a[gid] ~ dnorm( 0 , 1.5 )
    ),
  data=d )

precis( m11.7 , depth=2 )
```

```
##            mean         sd       5.5%      94.5%
## a[1] -0.2199869 0.03877483 -0.2819566 -0.1580173
## a[2] -0.8295337 0.05073355 -0.9106157 -0.7484517
```

```r
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

```
##             mean         sd      5.5%     94.5%
## diff_a 0.6107983 0.06422783 0.5067806 0.7115698
## diff_p 0.1416792 0.01445623 0.1181295 0.1642145
##                                                                                               histogram
## diff_a         <U+2581><U+2581><U+2581><U+2583><U+2587><U+2587><U+2585><U+2582><U+2581><U+2581><U+2581>
## diff_p <U+2581><U+2581><U+2581><U+2582><U+2583><U+2587><U+2587><U+2585><U+2582><U+2581><U+2581><U+2581>
```


```r
postcheck( m11.7 , n=1e4 )
# draw lines connecting points from same dept
d$dept_id <- rep( 1:6 , each=2 )
for ( i in 1:6 ) {
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x+1]/d$applications[x+1]
  lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
  text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-20-1.png)<!-- -->


```r
m11.8 <- quap(
  alist(
    admit ~ dbinom(applications , p) ,
    logit(p) <- a[gid] + delta[dept_id] ,
    a[gid] ~ dnorm(0 , 1.5) ,
    delta[dept_id] ~ dnorm(0 , 1.5)
    ),
  data = d)
precis(m11.8 , depth = 2)
```

```
##                mean        sd       5.5%      94.5%
## a[1]     -0.5278981 0.5322783 -1.3785816  0.3227855
## a[2]     -0.4312103 0.5330873 -1.2831867  0.4207662
## delta[1]  1.1080063 0.5350322  0.2529214  1.9630911
## delta[2]  1.0632109 0.5371968  0.2046667  1.9217552
## delta[3] -0.1502516 0.5347763 -1.0049275  0.7044242
## delta[4] -0.1826679 0.5350902 -1.0378453  0.6725096
## delta[5] -0.6246564 0.5378490 -1.4842429  0.2349302
## delta[6] -2.1727187 0.5468627 -3.0467109 -1.2987264
```


```r
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

```
##               mean         sd        5.5%       94.5%
## diff_a -0.09759870 0.08099147 -0.22698273 0.032906744
## diff_p -0.02186327 0.01849595 -0.05196188 0.007142185
##                                                                                                       histogram
## diff_a <U+2581><U+2581><U+2581><U+2581><U+2582><U+2585><U+2587><U+2587><U+2585><U+2582><U+2581><U+2581><U+2581>
## diff_p                                 <U+2581><U+2581><U+2581><U+2582><U+2587><U+2587><U+2582><U+2581><U+2581>
```


```r
pg <- sapply( 1:6 , function(k)
  d$applications[d$dept_id==k]/sum(d$applications[d$dept_id==k]) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )
```

```
##           A    B    C    D    E    F
## male   0.88 0.96 0.35 0.53 0.33 0.52
## female 0.12 0.04 0.65 0.47 0.67 0.48
```


```r
postcheck(m11.8)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-24-1.png)<!-- -->

```r
pairs(m11.8)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-24-2.png)<!-- -->

### 11.1.5. Multinomial and categorical models


```r
# simulate career choices among 500 individuals
N <- 500 # number of individuals
income <- 1:3 # expected income of each career
score <- 0.5*income # scores for each career, based on income
# next line converts scores to probabilities
p <- softmax(score[1],score[2],score[3])
# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA,N) # empty vector of choices for each individual
# sample chosen career for each individual
for ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )

career
```

```
##   [1] 3 3 1 1 2 2 3 2 3 3 3 3 1 2 1 3 3 3 3 2 3 3 3 2 3 3 3 2 1 3 2 3 3 1 1
##  [36] 3 1 3 3 1 2 2 2 3 2 1 3 2 2 2 3 1 2 3 3 3 3 2 2 2 1 1 2 2 1 3 2 2 2 3
##  [71] 2 2 3 2 1 2 2 2 3 3 2 3 3 2 3 2 3 3 3 1 3 3 1 2 1 3 3 1 3 3 3 3 3 3 2
## [106] 1 3 1 3 3 3 1 1 2 3 3 2 1 3 3 3 2 3 1 2 1 3 3 3 3 3 1 3 3 3 3 3 1 2 2
## [141] 3 2 2 2 2 1 3 2 3 2 3 3 1 3 1 2 2 1 2 1 3 3 3 2 3 3 1 3 2 3 2 2 2 3 2
## [176] 3 3 3 1 2 3 2 2 3 2 3 3 3 2 3 3 3 3 2 3 2 3 2 2 2 3 3 3 1 2 3 1 2 3 3
## [211] 2 3 3 2 3 3 3 3 3 2 2 2 3 1 1 3 1 2 2 2 3 3 3 3 3 3 3 3 3 2 3 3 3 1 2
## [246] 3 3 3 2 3 2 1 3 2 3 3 3 2 1 1 1 2 3 3 2 2 3 3 3 2 3 3 1 2 2 3 3 3 1 3
## [281] 1 3 3 3 2 3 2 3 3 1 2 3 1 2 3 3 2 3 3 2 3 3 3 1 2 2 2 1 3 2 1 2 3 2 3
## [316] 3 2 1 3 3 3 3 3 3 2 2 3 3 3 2 2 1 3 1 2 2 1 3 3 3 1 1 2 2 2 3 3 2 3 1
## [351] 3 3 1 3 3 2 3 3 3 2 1 3 1 2 3 2 2 1 3 2 1 2 3 3 3 3 2 2 2 1 2 3 1 1 3
## [386] 3 1 3 1 3 1 3 1 3 3 3 2 1 2 2 2 2 3 2 2 3 3 2 2 1 2 2 2 2 2 2 2 3 3 3
## [421] 2 2 2 2 2 3 3 2 3 3 3 3 1 3 1 2 2 3 3 2 3 1 3 1 3 3 2 2 2 3 2 3 3 3 2
## [456] 3 3 2 3 1 3 2 3 2 3 3 3 1 2 1 1 3 1 3 1 3 2 3 3 2 2 1 2 2 3 1 3 2 3 1
## [491] 2 1 2 3 3 3 2 3 1 2
```

```r
# fit the model, using dcategorical and softmax link
m10.16 <- map(
  alist(
    career ~ dcategorical( softmax(0,s2,s3) ),
    s2 <- b*2, # linear model for event type 2
    s3 <- b*3, # linear model for event type 3
    b ~ dnorm(0,5)
    ),
  data=list(career=career))
```


```r
N <- 100
# simulate family incomes for each individual
family_income <- runif(N)
# assign a unique coefficient for each type of event
b <- (1:-1)
career <- rep(NA,N) # empty vector of choices for each individual
for ( i in 1:N ) {
  score <- 0.5*(1:3) + b*family_income[i]
  p <- softmax(score[1],score[2],score[3])
  career[i] <- sample( 1:3 , size=1 , prob=p )
  }

m10.17 <- map(
  alist(
    career ~ dcategorical( softmax(0,s2,s3) ),
    s2 <- a2 + b2*family_income,
    s3 <- a3 + b3*family_income,
    c(a2,a3,b2,b3) ~ dnorm(0,5)
    ),
  data=list(career=career,family_income=family_income)
  )

m10.17
```

```
## 
## Quadratic approximate posterior distribution
## 
## Formula:
## career ~ dcategorical(softmax(0, s2, s3))
## s2 <- a2 + b2 * family_income
## s3 <- a3 + b3 * family_income
## c(a2, a3, b2, b3) ~ dnorm(0, 5)
## 
## Posterior means:
##         a2         a3         b2         b3 
## -0.5744676  0.5371752  0.3721305 -1.9435155 
## 
## Log-likelihood: -104.43
```

## 11.2. Poisson regression


```r
y <- rbinom(1e5,1000,1/1000)
c( mean(y) , var(y) )
```

```
## [1] 1.001370 1.003398
```

### 11.2.1. Example: Oceanic tool complexity


```r
data(Kline)
d <- Kline
d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )
d
```

```
##       culture population contact total_tools mean_TU            P
## 1    Malekula       1100     low          13     3.2 -1.291473310
## 2     Tikopia       1500     low          22     4.7 -1.088550750
## 3  Santa Cruz       3600     low          24     4.0 -0.515764892
## 4         Yap       4791    high          43     5.0 -0.328773359
## 5    Lau Fiji       7400    high          33     5.0 -0.044338980
## 6   Trobriand       8000    high          19     4.0  0.006668287
## 7       Chuuk       9200    high          40     3.8  0.098109204
## 8       Manus      13000     low          28     6.6  0.324317564
## 9       Tonga      17500    high          55     5.4  0.518797917
## 10     Hawaii     275000     low          71     6.6  2.321008320
##    contact_id
## 1           1
## 2           1
## 3           1
## 4           2
## 5           2
## 6           2
## 7           2
## 8           1
## 9           2
## 10          1
```


```r
curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-29-1.png)<!-- -->

```r
a <- rnorm(1e4,0,10)
lambda <- exp(a)
mean( lambda )
```

```
## [1] 2.405035e+13
```


```r
curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200, col = "black")
curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200, col = "blue", add = T)
curve( dlnorm( x , 3 , 1 ) , from=0 , to=100 , n=200, col = "red", add = T)
curve( dlnorm( x , 3 , 0.25 ) , from=0 , to=100 , n=200, col = "purple", add = T)
curve( dlnorm( x , 4 , 1 ) , from=0 , to=100 , n=200, col = "green", add = T)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-30-1.png)<!-- -->


```r
N <- 100
a <- rnorm(N , 3 , 0.5)
b <- rnorm(N , 0 , 10)
plot(NULL , xlim = c(-2, 2) , ylim = c(0, 100))
for (i in 1:N){
  curve(exp(a[i] + b[i] * x) , add = TRUE , col = col.alpha("black", 0.5))
}

a <- rnorm(N , 3 , 0.5)
b <- rnorm(N , 0 , 1)
for (i in 1:N){
  curve(exp(a[i] + b[i] * x) , add = TRUE , col = col.alpha("blue", 0.5))
}

a <- rnorm(N , 3 , 0.5)
b <- rnorm(N , 0 , 0.5)
for (i in 1:N){
  curve(exp(a[i] + b[i] * x) , add = TRUE , col = col.alpha("red", 0.5))
}

a <- rnorm(N , 3 , 0.5)
b <- rnorm(N , 0 , 0.2)
for (i in 1:N){
  curve(exp(a[i] + b[i] * x) , add = TRUE , col = col.alpha("green", 0.5))
}
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-31-1.png)<!-- -->


```r
x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )
par(mfrow = c(1,2))
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab="log population" , ylab="total tools" )
for ( i in 1:N ){
  lines( x_seq , lambda[i,] , col=col.alpha("black",0.5) , lwd=1.5 )
}
plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" , ylab="total tools" )
for ( i in 1:N ){
  lines( exp(x_seq) , lambda[i,] , col=col.alpha("black",0.5) , lwd=1.5 )
}
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-32-1.png)<!-- -->


```r
dat <- list(T = d$total_tools ,
            P = d$P ,
            cid = d$contact_id)
# intercept only
m11.9 <- ulam(
  alist(
    T ~ dpois(lambda),
    log(lambda) <- a,
    a ~ dnorm(3, 0.5)
    ), data = dat, chains = 4, log_lik = TRUE
)
```

```
## 
## SAMPLING FOR MODEL '1016dad00d71412e9dffb1c8f58132a3' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.007 seconds (Warm-up)
## Chain 1:                0.006 seconds (Sampling)
## Chain 1:                0.013 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '1016dad00d71412e9dffb1c8f58132a3' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.008 seconds (Warm-up)
## Chain 2:                0.008 seconds (Sampling)
## Chain 2:                0.016 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '1016dad00d71412e9dffb1c8f58132a3' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.007 seconds (Warm-up)
## Chain 3:                0.007 seconds (Sampling)
## Chain 3:                0.014 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '1016dad00d71412e9dffb1c8f58132a3' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.008 seconds (Warm-up)
## Chain 4:                0.007 seconds (Sampling)
## Chain 4:                0.015 seconds (Total)
## Chain 4:
```

```r
# interaction model
m11.10 <- ulam(
  alist(
    T ~ dpois(lambda),
    log(lambda) <- a[cid] + b[cid] * P,
    a[cid] ~ dnorm(3 , 0.5),
    b[cid] ~ dnorm(0 , 0.2)
  ), data = dat, chains = 4, log_lik = TRUE
)
```

```
## 
## SAMPLING FOR MODEL '764b6f1d9a7aa55a03b1520aed83f315' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.021 seconds (Warm-up)
## Chain 1:                0.016 seconds (Sampling)
## Chain 1:                0.037 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '764b6f1d9a7aa55a03b1520aed83f315' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.02 seconds (Warm-up)
## Chain 2:                0.018 seconds (Sampling)
## Chain 2:                0.038 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '764b6f1d9a7aa55a03b1520aed83f315' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.02 seconds (Warm-up)
## Chain 3:                0.019 seconds (Sampling)
## Chain 3:                0.039 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '764b6f1d9a7aa55a03b1520aed83f315' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.02 seconds (Warm-up)
## Chain 4:                0.019 seconds (Sampling)
## Chain 4:                0.039 seconds (Total)
## Chain 4:
```

```r
compare( m11.9 , m11.10 , func=LOO )
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are high (>0.5). Use PSISk to inspect individual points.
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in log(z): NaNs produced
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are very high (>1). Use PSISk to inspect individual points.
```

```
##             PSIS pPSIS   dPSIS       weight       SE     dSE
## m11.10  86.11393     0  0.0000 1.000000e+00 13.35825      NA
## m11.9  142.16903     0 56.0551 6.726521e-13 34.21930 33.4386
```


```r
par(mfrow=c(1,1))
k <- PSISk(m11.10)
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are very high (>1). Use PSISk to inspect individual points.
```

```r
plot(
  dat$P ,
  dat$T ,
  xlab = "log population (std)" ,
  ylab = "total tools" ,
  col = rangi2 ,
  pch = ifelse(dat$cid == 1 , 1 , 16) ,
  lwd = 2 ,
  ylim = c(0, 75) ,
  cex = 1 + normalize(k)
)
# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq(from = -1.4 ,
             to = 3 ,
             length.out = ns)
# predictions for cid=1 (low contact)
lambda <- link(m11.10 , data = data.frame(P = P_seq , cid = 1))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(P_seq , lmu , lty = 2 , lwd = 1.5)
shade(lci , P_seq , xpd = TRUE)
# predictions for cid=2 (high contact)
lambda <- link(m11.10 , data = data.frame(P = P_seq , cid = 2))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(P_seq , lmu , lty = 1 , lwd = 1.5)
shade(lci , P_seq , xpd = TRUE)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-34-1.png)<!-- -->


```r
plot(
  d$population ,
  d$total_tools ,
  xlab = "population" ,
  ylab = "total tools" ,
  col = rangi2 ,
  pch = ifelse(dat$cid == 1 , 1 , 16) ,
  lwd = 2 ,
  ylim = c(0, 75) ,
  cex = 1 + normalize(k)
)
ns <- 100
P_seq <- seq(from = -5 ,
             to = 3 ,
             length.out = ns)
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp(P_seq * 1.53 + 9)
lambda <- link(m11.10 , data = data.frame(P = P_seq , cid = 1))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(pop_seq , lmu , lty = 2 , lwd = 1.5)
shade(lci , pop_seq , xpd = TRUE)
lambda <- link(m11.10 , data = data.frame(P = P_seq , cid = 2))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(pop_seq , lmu , lty = 1 , lwd = 1.5)
shade(lci , pop_seq , xpd = TRUE)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-35-1.png)<!-- -->

### Overthinking: Modeling tool innovation

```r
dat2 <-
  list(
    T = d$total_tools,
    P = d$population,
    cid = d$contact_id
  )

m11.11 <- ulam(
  alist(
    T ~ dpois(lambda),
    lambda <- exp(a[cid]) * P^b[cid]/g,
    a[cid] ~ dnorm(1, 1),
    b[cid] ~ dexp(1),
    g ~ dexp(1)
  ),
  data = dat2 ,
  chains = 4 ,
  log_lik = TRUE
)
```

```
## 
## SAMPLING FOR MODEL '1d9bfc94a8db83d885271311e6e47d39' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.284 seconds (Warm-up)
## Chain 1:                0.239 seconds (Sampling)
## Chain 1:                0.523 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL '1d9bfc94a8db83d885271311e6e47d39' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.696 seconds (Warm-up)
## Chain 2:                0.241 seconds (Sampling)
## Chain 2:                0.937 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL '1d9bfc94a8db83d885271311e6e47d39' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.305 seconds (Warm-up)
## Chain 3:                0.285 seconds (Sampling)
## Chain 3:                0.59 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL '1d9bfc94a8db83d885271311e6e47d39' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
## Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
## Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
## Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
## Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
## Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
## Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
## Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
## Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
## Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
## Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
## Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.376 seconds (Warm-up)
## Chain 4:                0.257 seconds (Sampling)
## Chain 4:                0.633 seconds (Total)
## Chain 4:
```

```r
k <- PSISk(m11.11)
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```
## Warning in xcheckLOOk(loo_list$diagnostics$pareto_k): Some Pareto k values
## are high (>0.5). Use PSISk to inspect individual points.
```

```r
plot(
  d$population ,
  d$total_tools ,
  xlab = "population" ,
  ylab = "total tools" ,
  col = rangi2 ,
  pch = ifelse(dat2$cid == 1 , 1 , 16) ,
  lwd = 2 ,
  ylim = c(0, 75) ,
  cex = 1 + normalize(k)
)
ns <- 100
P_seq <- seq(from = -5 ,
             to = 3 ,
             length.out = ns)
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp(P_seq * 1.53 + 9)
lambda <- link(m11.11 , data = data.frame(P = pop_seq , cid = 1))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(pop_seq , lmu , lty = 2 , lwd = 1.5)
shade(lci , pop_seq , xpd = TRUE)
lambda <- link(m11.11 , data = data.frame(P = pop_seq , cid = 2))
lmu <- apply(lambda , 2 , mean)
lci <- apply(lambda , 2 , PI)
lines(pop_seq , lmu , lty = 1 , lwd = 1.5)
shade(lci , pop_seq , xpd = TRUE)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-36-1.png)<!-- -->

### 11.2.2. Negative binomial (gamma-Poisson) models
### 11.2.3. Example: Exposure and the offset


```r
num_days <- 30
y <- rpois( num_days , 1.5 )
y
```

```
##  [1] 0 1 4 1 4 1 1 1 0 1 1 2 2 0 1 1 2 2 1 1 0 0 2 1 3 0 4 2 0 1
```

```r
num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 )
y_new
```

```
## [1] 2 4 1 0
```

```r
y_all <- c( y , y_new )
y_all
```

```
##  [1] 0 1 4 1 4 1 1 1 0 1 1 2 2 0 1 1 2 2 1 1 0 0 2 1 3 0 4 2 0 1 2 4 1 0
```

```r
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )
d
```

```
##    y days monastery
## 1  0    1         0
## 2  1    1         0
## 3  4    1         0
## 4  1    1         0
## 5  4    1         0
## 6  1    1         0
## 7  1    1         0
## 8  1    1         0
## 9  0    1         0
## 10 1    1         0
## 11 1    1         0
## 12 2    1         0
## 13 2    1         0
## 14 0    1         0
## 15 1    1         0
## 16 1    1         0
## 17 2    1         0
## 18 2    1         0
## 19 1    1         0
## 20 1    1         0
## 21 0    1         0
## 22 0    1         0
## 23 2    1         0
## 24 1    1         0
## 25 3    1         0
## 26 0    1         0
## 27 4    1         0
## 28 2    1         0
## 29 0    1         0
## 30 1    1         0
## 31 2    7         1
## 32 4    7         1
## 33 1    7         1
## 34 0    7         1
```

```r
# compute the offset
d$log_days <- log( d$days )
# fit the model

m11.12.1 <- quap(
  alist(
    y ~ dpois( lambda ),
    log(lambda) <- log_days + a + b*monastery,
    a ~ dnorm( 0 , 1 ),
    b ~ dnorm( 0 , 1 )
    ),
  data=d)

m11.12 <- ulam(
  alist(
    y ~ dpois( lambda ),
    log(lambda) <- log_days + a + b*monastery,
    a ~ dnorm( 0 , 1 ),
    b ~ dnorm( 0 , 1 )
    ),
  data=d[,-2], chains = 4, cores = 4, log_lik = T )

post <- extract.samples( m11.12.1 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )
```

```
##                 mean        sd      5.5%     94.5%
## lambda_old 1.2919896 0.2042846 0.9906811 1.6428000
## lambda_new 0.3165439 0.1052508 0.1774543 0.5062209
##                                                                                   histogram
## lambda_old         <U+2581><U+2581><U+2585><U+2587><U+2583><U+2581><U+2581><U+2581><U+2581>
## lambda_new <U+2581><U+2582><U+2587><U+2585><U+2582><U+2581><U+2581><U+2581><U+2581><U+2581>
```

```r
post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )
```

```
##                 mean         sd      5.5%     94.5%
## lambda_old 1.2797781 0.20348451 0.9757365 1.6203812
## lambda_new 0.3065288 0.09901634 0.1680730 0.4809097
##                                                                                                                   histogram
## lambda_old <U+2581><U+2581><U+2582><U+2585><U+2587><U+2587><U+2587><U+2585><U+2583><U+2582><U+2581><U+2581><U+2581><U+2581>
## lambda_new <U+2581><U+2581><U+2583><U+2587><U+2587><U+2587><U+2585><U+2582><U+2582><U+2581><U+2581><U+2581><U+2581><U+2581>
```
### 11.2.4. Multinomial in disguise as Poisson


```r
data(UCBadmit)
d <- UCBadmit

# binomial model of overall admission probability
set.seed(100)
m_binom <- quap(
  alist(
    admit ~ dbinom(applications,p),
    logit(p) <- a,
    a ~ dnorm(0,100)
    ),
  data=d )

# Poisson model of overall admission rate and rejection rate
d$rej <- d$reject # 'reject' is a reserved word
d$gender <- d$applicant.gender # no dots in names
m_pois <- ulam(
  alist(
    admit ~ dpois(lambda1),
    rej ~ dpois(lambda2),
    log(lambda1) <- a1,
    log(lambda2) <- a2,
    c(a1,a2) ~ dnorm(0,100)
    ),
  data=d[,-2] , chains=3 , cores=3 )
```

```
## Removing one or more character or factor variables:
```

```
## deptgender
```

```r
m_pois2 <- map2stan(
  alist(
    admit ~ dpois(lambda1),
    rej ~ dpois(lambda2),
    log(lambda1) <- a1,
    log(lambda2) <- a2,
    c(a1,a2) ~ dnorm(0,100)
    ),
  data=d, chains=3 , cores=3 )
```

```
## Warning: Variable 'applicant.gender' contains dots '.'.
## Will attempt to remove dots internally.
```

```
## Computing WAIC
```

```r
logistic(coef(m_binom))
```

```
##         a 
## 0.3877596
```

```r
logistic(coef(m_pois))
```

```
##        a2        a1 
## 0.9956888 0.9932085
```

```r
logistic(coef(m_pois2))
```

```
##        a1        a2 
## 0.9932072 0.9956875
```

```r
precis(m_binom, depth = 2)
```

```
##         mean         sd       5.5%      94.5%
## a -0.4567394 0.03050707 -0.5054956 -0.4079832
```

```r
precis(m_pois, depth = 2)
```

```
##       mean         sd     5.5%    94.5%    n_eff      Rhat
## a2 5.44221 0.01906197 5.412242 5.472154 1097.739 1.0003408
## a1 4.98527 0.02350805 4.948440 5.022987 1109.932 0.9996039
```

```r
precis(m_pois2, depth = 2)
```

```
##        mean         sd     5.5%    94.5%    n_eff     Rhat
## a1 4.985079 0.02427899 4.945648 5.023461 2105.349 1.000178
## a2 5.441913 0.01848007 5.412261 5.471651 1990.272 1.000223
```

```r
k <- as.numeric(coef(m_pois))
k
```

```
## [1] 5.44221 4.98527
```

```r
exp(k[1])/(exp(k[1])+exp(k[2]))
```

```
## [1] 0.612288
```

```r
k <- as.numeric(coef(m_pois2))
k
```

```
## [1] 4.985079 5.441913
```

```r
exp(k[1])/(exp(k[1])+exp(k[2]))
```

```
## [1] 0.387737
```
### Overthinking: Multinomial-Poisson transformation

## 11.3. Censoring and survival
### Overthinking: Generative models for exponential and gamma distributions


```r
N <- 5
x <- replicate( 1e5 , min(runif(N,1,100)) )
dens(x, col = "blue")
N <- 2
x <- replicate( 1e5 , min(runif(N,1,100)) )
dens(x, add = T)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-39-1.png)<!-- -->

```r
N <- 10
M <- 2
x <- replicate( 1e5 , sort(runif(N,1,100))[M])
dens(x)
N <- 10
M <- 5
x <- replicate( 1e5 , sort(runif(N,1,100))[M])
dens(x, col = "blue", add = T)
```

![](11_8_2019_Notes_files/figure-html/unnamed-chunk-39-2.png)<!-- -->

### 11.3.1. Simulated cats
### 11.3.2. Actual cats


```r
data(AustinCats)
d <- AustinCats
d$adopt <- ifelse(d$out_event == "Adoption" , 1L , 0L)
dat <- list(
  days_to_event = as.numeric(d$days_to_event),
  color_id = ifelse(d$color == "Black" , 1L , 2L) ,
  adopted = d$adopt
)
m11.14 <- ulam(
  alist(
    days_to_event | adopted == 1 ~ exponential(lambda),
    days_to_event | adopted == 0 ~ custom(exponential_lccdf(!Y |
                                                              lambda)),
    lambda <- 1.0 / mu,
    log(mu) <- a[color_id],
    a[color_id] ~ normal(0, 1)
  ),
  data = dat ,
  chains = 4 ,
  cores = 4
)
precis(m11.14 , 2)
```

```
##          mean          sd     5.5%    94.5%    n_eff     Rhat
## a[1] 4.051131 0.025276966 4.011798 4.091351 1343.607 1.000006
## a[2] 3.880125 0.009892881 3.864538 3.896008 1425.825 0.999235
```

```r
post <- extract.samples( m11.14 )
post$D <- exp(post$a)
precis( post , 2 )
```

```
##           mean          sd      5.5%     94.5%
## a[1]  4.051131 0.025276966  4.011798  4.091351
## a[2]  3.880125 0.009892881  3.864538  3.896008
## D[1] 57.480758 1.453895200 55.246087 59.820643
## D[2] 48.432658 0.479269836 47.681248 49.205647
##                                                                                                                     histogram
## a[1]                                                 <U+2581><U+2581><U+2582><U+2585><U+2587><U+2587><U+2582><U+2581><U+2581>
## a[2] <U+2581><U+2581><U+2581><U+2582><U+2583><U+2585><U+2587><U+2587><U+2585><U+2583><U+2582><U+2581><U+2581><U+2581><U+2581>
## D[1]                                 <U+2581><U+2581><U+2581><U+2583><U+2585><U+2587><U+2585><U+2582><U+2581><U+2581><U+2581>
## D[2]                                                         <U+2581><U+2581><U+2583><U+2587><U+2587><U+2582><U+2581><U+2581>
```

## 11.4. Summary
