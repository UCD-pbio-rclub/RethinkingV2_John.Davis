---
title: "10_11_2019_HW"
author: "John D."
date: "October 10, 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=native')
```

## 8E4. Explain the difference between the effective number of samples, n_eff as calculated by Stan, and the actual number of samples.

The effective number of samples is an estimate of the number of independent samples from the posterior distribution and the actual number of samples is the raw count.

## 8M3. Re-estimate one of the Stan models from the chapter, but at different numbers of warmup iterations. Be sure to use the same number of sampling iterations in each case. Compare the n_eff values. How much warmup is enough?

```{r}
# data prep
data(rugged)
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[ complete.cases(d$rgdppc_2000) , ]
dd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)
dd$rugged_std <- dd$rugged / max(dd$rugged)
dd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )
dat_slim <- list(
  log_gdp_std = dd$log_gdp_std,
  rugged_std = dd$rugged_std,
  cid = as.integer( dd$cid )
)
```

```{r}
m9.1.10 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 10, log_lik = T)

m9.1.50 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 50, log_lik = T)

m9.1.100 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 100, log_lik = T)

m9.1.200 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 200, log_lik = T)

m9.1.500 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 500, log_lik = T)

m9.1.900 <- ulam(
  alist(
    log_gdp_std ~ dnorm( mu , sigma ) ,
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,
    a[cid] ~ dnorm( 1 , 0.1 ) ,
    b[cid] ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
    ) ,
data=dat_slim , chains=1, iter = 1000, warmup = 900, log_lik = T)

warmups <- c(10,50,100,200,500,900)
n_effs <- c(mean(precis(m9.1.10,depth = 2)[,5]),
            mean(precis(m9.1.50,depth = 2)[,5]),
            mean(precis(m9.1.100,depth = 2)[,5]),
            mean(precis(m9.1.200,depth = 2)[,5]),
            mean(precis(m9.1.500,depth = 2)[,5]),
            mean(precis(m9.1.900,depth = 2)[,5]))
n_effs
plot(x = warmups, y = n_effs)
lines(x = warmups, y = n_effs)
```
In this case it looks like using 200 iterations produced the largest number of effective samples

## 8H3. Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Hereâ€™s an example to work and think through. ### Go back to the leg length example in Chapter 5. Here is the code again, which simulates height and leg lengths for 100 imagined individuals:

```{r}
N <- 100 # number of individuals
height <- rnorm(N,10,2) # sim total height of each
leg_prop <- runif(N,0.4,0.5) # leg as proportion of height
leg_left <- leg_prop*height + # sim left leg as proportion + error
  rnorm( N , 0 , 0.02 )
leg_right <- leg_prop*height + # sim right leg as proportion + error
  rnorm( N , 0 , 0.02 )
# combine into data frame
d <- data.frame(height,leg_left,leg_right)
head(d)
```

```{r}
m5.8s <- map2stan(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left + br*leg_right ,
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    br ~ dnorm( 2 , 10 ) ,
    sigma ~ dcauchy( 0 , 1 )
  ) ,
  data=d, chains=4, cores = 4,
  start=list(a=10,bl=0,br=0,sigma=1) )
```

```{r}
m5.8s2 <- map2stan(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + bl*leg_left + br*leg_right ,
    a ~ dnorm( 10 , 100 ) ,
    bl ~ dnorm( 2 , 10 ) ,
    br ~ dnorm( 2 , 10 ) & T[0,] ,
    sigma ~ dcauchy( 0 , 1 )
  ) ,
  data=d, chains=4, cores = 4,
  start=list(a=10,bl=0,br=0,sigma=1) )
```

```{r}
plot(m5.8s)
precis(m5.8s, depth = 2)
pairs(m5.8s)
```
```{r}
plot(m5.8s2)
precis(m5.8s2, depth = 2)
pairs(m5.8s)
```
```{r}
coeftab(m5.8s, m5.8s2)
compare(m5.8s, m5.8s2)
```

```{r}
samples1 <- data.frame(extract.samples(m5.8s)) %>%
  mutate(Model = 1)
samples2 <- data.frame(extract.samples(m5.8s2)) %>%
  mutate(Model = 2)
mean(samples1$bl + samples1$br)
mean(samples2$bl + samples2$br)

par(mfrow = c(2,2))
dens(samples1$bl)
dens(samples2$bl)
dens(samples1$br)
dens(samples2$br)
```

By making br always positive, it forced bl to be always negative. Originally both were centered around 0, but now both distributions are on opposite sides of 0. They still carry the same sum though.

## 8H4. For the two models fit in the previous problem, use DIC or WAIC to compare the effective numbers of parameters for each model. Which model has more effective parameters? Why?

```{r}
compare(m5.8s,m5.8s2)
```

The effective number of parameters is smalller in the second model. This is most likely due to us constraining the br parameter thus reducing its variance and therefore reducing pWAIC
